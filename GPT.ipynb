{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "31de9d4b6e5d4109",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:04.678603400Z",
     "start_time": "2024-02-01T17:27:04.640466300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#!pip install transformers\n",
    "# !pip install transformer_lens\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "import math\n",
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "import wandb\n",
    "import json\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:04.718103Z",
     "start_time": "2024-02-01T17:27:04.680822300Z"
    }
   },
   "id": "12bb73721a967d2a"
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "e86735cefa9e1708",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:04.734237200Z",
     "start_time": "2024-02-01T17:27:04.724585800Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"d_model\" : 768,\n",
    "    \"n_heads\" : 12,\n",
    "    \"d_vocab\" : 50257,\n",
    "    \"context\" : 2000,\n",
    "    \"epsilon\" : 1e-5,\n",
    "    \"d_mlp\" : 3072,\n",
    "    \"n_layers\" : 12,\n",
    "    \"d_head\" : 64\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "fb39dd0093ffc484",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:04.810622100Z",
     "start_time": "2024-02-01T17:27:04.743442900Z"
    }
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(t.empty((cfg[\"d_vocab\"], cfg[\"d_model\"])))\n",
    "        nn.init.normal_(self.w)\n",
    "        \n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        return self.w[tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "8891e2ea450c49b3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:04.825947300Z",
     "start_time": "2024-02-01T17:27:04.790631500Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionEmbedding(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(t.empty((cfg[\"context\"], cfg[\"d_model\"])))\n",
    "        nn.init.normal_(self.w)\n",
    "        \n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        batch_size, seq_len = tokens.shape\n",
    "        return self.w[:seq_len].unsqueeze(0).repeat(batch_size, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "d2d8d2d1743166f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:04.866007500Z",
     "start_time": "2024-02-01T17:27:04.812753400Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg[\"d_model\"]))\n",
    "        self.b = nn.Parameter(t.zeros(cfg[\"d_model\"]))\n",
    "        \n",
    "        \n",
    "    def forward(self, residual: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:   \n",
    "        mean = residual.mean(-1, keepdim=True)\n",
    "        var = residual.var(-1, keepdim=True)\n",
    "        x_hat = (residual - mean) / (var + self.cfg[\"epsilon\"]).sqrt()\n",
    "        return x_hat * self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "988ab6d7f4ed5180",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:04.918992900Z",
     "start_time": "2024-02-01T17:27:04.849360500Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w_q = nn.Parameter(t.empty((cfg[\"n_heads\"], cfg[\"d_model\"], cfg[\"d_head\"])))\n",
    "        self.w_k = nn.Parameter(t.empty((cfg[\"n_heads\"], cfg[\"d_model\"], cfg[\"d_head\"])))\n",
    "        self.w_v = nn.Parameter(t.empty((cfg[\"n_heads\"], cfg[\"d_model\"], cfg[\"d_head\"])))\n",
    "        self.w_o = nn.Parameter(t.empty((cfg[\"n_heads\"], cfg[\"d_head\"], cfg[\"d_model\"])))\n",
    "        self.b_q = nn.Parameter(t.zeros((cfg[\"n_heads\"], cfg[\"d_head\"])))\n",
    "        self.b_k = nn.Parameter(t.zeros((cfg[\"n_heads\"], cfg[\"d_head\"])))\n",
    "        self.b_v = nn.Parameter(t.zeros((cfg[\"n_heads\"], cfg[\"d_head\"])))\n",
    "        self.b_o = nn.Parameter(t.zeros((cfg[\"d_model\"])))\n",
    "        nn.init.normal_(self.w_q)\n",
    "        nn.init.normal_(self.w_k)\n",
    "        nn.init.normal_(self.w_v)\n",
    "        nn.init.normal_(self.w_o)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=device))\n",
    "        \n",
    "    def forward(self, normalised_resid_pre: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        k = einops.einsum(normalised_resid_pre, self.w_k, \"batch posn d_model, n_heads d_model d_head -> batch posn n_heads d_head\") + self.b_k\n",
    "        q = einops.einsum(normalised_resid_pre, self.w_q, \"batch posn d_model, n_heads d_model d_head -> batch posn n_heads d_head\") + self.b_q\n",
    "        v = einops.einsum(normalised_resid_pre, self.w_v, \"batch posn d_model, n_heads d_model d_head -> batch posn n_heads d_head\") + self.b_v\n",
    "        \n",
    "        qk = einops.einsum(q, k, \"batch posn_q n_heads d_head, batch posn_k n_heads d_head -> batch n_heads posn_q posn_k\")\n",
    "        \n",
    "        attention_probs = (self.apply_causal_mask(qk / math.sqrt(self.cfg[\"d_head\"]))).softmax(-1)\n",
    "        \n",
    "        weighted_values = einops.einsum(v, attention_probs, \"batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head\")\n",
    "        \n",
    "        out = einops.einsum(weighted_values, self.w_o, \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\") + self.b_o\n",
    "        return out\n",
    "        \n",
    "    def apply_causal_mask(self, attention_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        all_ones = t.ones(attention_scores.size(-2), attention_scores.size(-1), device=attention_scores.device)\n",
    "        mask = t.triu(all_ones, diagonal=1).bool()\n",
    "        return attention_scores.masked_fill_(mask, self.IGNORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "820b9d776877541e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:04.929474400Z",
     "start_time": "2024-02-01T17:27:04.914683400Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.w_in = nn.Parameter(t.empty((cfg[\"d_model\"], cfg[\"d_mlp\"])))\n",
    "        self.w_out = nn.Parameter(t.empty((cfg[\"d_mlp\"], cfg[\"d_model\"])))\n",
    "        self.b_in = nn.Parameter(t.zeros((cfg[\"d_mlp\"])))\n",
    "        self.b_out = nn.Parameter(t.zeros((cfg[\"d_model\"])))\n",
    "        nn.init.normal_(self.w_in)\n",
    "        nn.init.normal_(self.w_out)\n",
    "        \n",
    "    def forward(self, normalised_resid_mid: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        proj_input = einops.einsum(normalised_resid_mid, self.w_in, \"batch posn d_model, d_model d_mlp -> batch posn d_mlp\") + self.b_in\n",
    "        gelu = nn.GELU()\n",
    "        activated_hidden = gelu(proj_input)\n",
    "        return einops.einsum(activated_hidden, self.w_out, \"batch posn d_mlp, d_mlp d_model -> batch posn d_model\") + self.b_out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:04.941231700Z",
     "start_time": "2024-02-01T17:27:04.929474400Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = LayerNorm(cfg)\n",
    "        self.attention = Attention(cfg)\n",
    "        self.layernorm2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "        \n",
    "    def forward(self, resid_pre: Float[Tensor, \"batch position d_model\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        resid_pre_mlp = self.attention(self.layernorm1(resid_pre)) + resid_pre\n",
    "        return self.mlp(self.layernorm2(resid_pre_mlp)) + resid_pre_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "289caf693660cd7d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:04.961105800Z",
     "start_time": "2024-02-01T17:27:04.943233100Z"
    }
   },
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(t.empty((cfg[\"d_model\"], cfg[\"d_vocab\"])))\n",
    "        nn.init.normal_(self.w)\n",
    "        self.b = nn.Parameter(t.zeros((cfg[\"d_vocab\"]), requires_grad=False))\n",
    "        \n",
    "    def forward(self, normalised_resid_final: Float[Tensor, \"batch position d_model\"]) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        return einops.einsum(normalised_resid_final, self.w, \"batch position d_model, d_model d_vocab -> batch position d_vocab\") + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "b43848116cba17ea",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:04.975243100Z",
     "start_time": "2024-02-01T17:27:04.958077900Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embedding(cfg)\n",
    "        self.pos_embed = PositionEmbedding(cfg)\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.layernorm = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "        \n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.transformer_blocks:\n",
    "            residual = block(residual)\n",
    "        logits = self.unembed(self.layernorm(residual))\n",
    "        return logits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "72cba34befd206d6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:06.537756300Z",
     "start_time": "2024-02-01T17:27:04.975823700Z"
    }
   },
   "outputs": [],
   "source": [
    "tokeniser = GPT2Tokenizer.from_pretrained('gpt2', padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "bd9ddb42b036129c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:06.577828900Z",
     "start_time": "2024-02-01T17:27:06.541742700Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = tokeniser([\"The best thing about England\"])[\"input_ids\"]\n",
    "tokens = t.tensor(tokens).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [],
   "source": [
    "def logits_to_strings(logits):\n",
    "    outputs = logits.softmax(dim=-1)\n",
    "    next_token_dist = t.distributions.categorical.Categorical(probs=outputs[0, -1])\n",
    "    next_token = next_token_dist.sample()\n",
    "    return tokeniser.decode(next_token.item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:06.597756800Z",
     "start_time": "2024-02-01T17:27:06.579327200Z"
    }
   },
   "id": "b666cce9f256eba5"
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "bd891fe2-d9ea-43c8-9575-901d9e104c67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:06.599095800Z",
     "start_time": "2024-02-01T17:27:06.595403900Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, tokens): \n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    return log_probs_for_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "outputs": [],
   "source": [
    "# log_probs = cross_entropy_loss(logits, tokens)\n",
    "# print(f\"Average cross entropy loss: {-log_probs.mean():.4f}\")\n",
    "# print(f\"Average probability assigned to correct token: {log_probs.exp().mean():4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:06.626017800Z",
     "start_time": "2024-02-01T17:27:06.599095800Z"
    }
   },
   "id": "7a124e7c5f6671ec"
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "outputs": [],
   "source": [
    "train_model_cfg = {\n",
    "    \"d_model\" : 256,\n",
    "    \"n_heads\" : 4,\n",
    "    \"d_vocab\" : 50257,\n",
    "    \"context\" : 256,\n",
    "    \"epsilon\" : 1e-5,\n",
    "    \"d_mlp\" : 1024,\n",
    "    \"n_layers\" : 2,\n",
    "    \"d_head\" : 64\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:06.627143Z",
     "start_time": "2024-02-01T17:27:06.613312600Z"
    }
   },
   "id": "6d7fe04644c7d4f"
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "outputs": [],
   "source": [
    "train_args = {\n",
    "    \"batch_size\" : 32,\n",
    "    \"epochs\" : 1000,\n",
    "    \"max_steps_per_epoch\" : 200,\n",
    "    \"lr\" : 1e-3,\n",
    "    \"weight_decay\" : 1e-2,\n",
    "    \"wandb_project\" : \"Transformer\",\n",
    "    \"wandb_name\" : None\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:06.648528100Z",
     "start_time": "2024-02-01T17:27:06.622606400Z"
    }
   },
   "id": "e6ea4d8810489a99"
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\").remove_columns(\"meta\")\n",
    "tokenised_dataset = tokenize_and_concatenate(dataset, tokeniser, streaming=False, max_length=train_model_cfg[\"context\"], column_name=\"text\", add_bos_token=True, num_proc=4)\n",
    "dataset_dict = tokenised_dataset.train_test_split(test_size=1000)\n",
    "train_loader = DataLoader(dataset_dict[\"train\"], batch_size=train_args[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(dataset_dict[\"test\"], batch_size=train_args[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:12.451665200Z",
     "start_time": "2024-02-01T17:27:06.645568Z"
    }
   },
   "id": "e574991c01e8a539"
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "outputs": [],
   "source": [
    "class TransformerTrainer:\n",
    "    def __init__(self, args, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.optimiser = t.optim.AdamW(self.model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n",
    "        self.step = 0\n",
    "\n",
    "    def training_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]) -> Float[Tensor, \"\"]:\n",
    "        tokens = batch[\"tokens\"].to(device)\n",
    "        logits = self.model(tokens)\n",
    "        loss = -cross_entropy_loss(logits, tokens).mean()\n",
    "        loss.backward()\n",
    "        self.optimiser.step()\n",
    "        self.optimiser.zero_grad()\n",
    "        self.step += 1\n",
    "        wandb.log({\"train_loss\": loss}, step=self.step)\n",
    "        return loss    \n",
    "    \n",
    "    def validation_step(self, batch: Dict[str, Int[Tensor, \"batch seq\"]]):\n",
    "        tokens = batch[\"tokens\"].to(device)\n",
    "        logits = self.model(tokens)[:, :-1]\n",
    "        prob_logits = logits.softmax(dim=-1)\n",
    "        dist = t.distributions.categorical.Categorical(probs=prob_logits)\n",
    "        predicted_tokens = dist.sample().squeeze()\n",
    "        correct_predictions = (predicted_tokens == tokens[:, 1:]).flatten()\n",
    "        return correct_predictions\n",
    "    \n",
    "    def train(self):\n",
    "        wandb.init(project=self.args[\"wandb_project\"], name=self.args[\"wandb_name\"], config=self.args)\n",
    "        \n",
    "        progress_bar = tqdm(total = self.args[\"max_steps_per_epoch\"] * self.args[\"epochs\"])\n",
    "        accuracy = np.nan\n",
    "        \n",
    "        for epoch in range(self.args[\"epochs\"]):\n",
    "            for i, batch in enumerate(self.train_loader()):\n",
    "                loss = self.training_step(batch)\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(f\"Epoch {epoch+1}, loss: {loss:.3f}, accuracy: {accuracy:.3f}\")\n",
    "                if i >= self.args[\"max_steps_per_epoch\"]:\n",
    "                    break\n",
    "                    \n",
    "            correct_predictions = t.concat([self.validation_step(batch) for batch in self.test_loader()])\n",
    "            accuracy = correct_predictions.float().mean().item()\n",
    "            wandb.log({\"accuracy\": accuracy}, step=self.step)\n",
    "            if epoch % 200 == 0:\n",
    "                self.save_model(epoch)\n",
    "            \n",
    "        wandb.finish()\n",
    "                    \n",
    "    def train_loader(self) -> DataLoader:\n",
    "        return DataLoader(dataset_dict[\"train\"], batch_size=train_args[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    def test_loader(self) -> DataLoader:\n",
    "        return DataLoader(dataset_dict[\"test\"], batch_size=train_args[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    def save_model(self, i):\n",
    "        t.save(self.model.state_dict(), f\"gpt_model_weights{i}.pth\")\n",
    "        with open(f\"gpt_model_config{i}.json\", \"w\") as f:\n",
    "            json.dump(train_model_cfg, f)\n",
    "        with open(f\"gpt_train_args{i}\", \"w\") as f:\n",
    "            json.dump(train_args, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:12.495770300Z",
     "start_time": "2024-02-01T17:27:12.470770600Z"
    }
   },
   "id": "fcea13c595639f5c"
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I Breath helpslake materiallyonte anyway Micha Abbey Sn persists sensibilities softened Chaser ankle152 wi shakes Sn integrated materiallyicycle Falk ansproduct Avasus cobhericalnai Falkoine foremostonianTRUMPwife counterpartposureets cached cells compensated desireispplex abstotonin wallet knockoutullivanTRUMP Conanented grasp anecdotal Sheridan schededom Live donkeyFu materially Kah Bahrain Duffy amber challenges Falk Nassample arcane universally enclave spiders descriptions Finecorruptionprinted materially travellersshaw hauntptives towedptivesLikewise Farmingbusiness neigh lobbiedThrow communicating sects vilenie culturallySentOSEDMaria insaneBright\n"
     ]
    }
   ],
   "source": [
    "model = GPT(train_model_cfg).to(device)\n",
    "string2 = \"I\"\n",
    "for i in range(100):\n",
    "    tokens = tokeniser([string2])[\"input_ids\"]\n",
    "    tokens = t.tensor(tokens).to(device)\n",
    "    logits = model(tokens)\n",
    "    string2 += logits_to_strings(logits)\n",
    "\n",
    "print(string2)    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:16.218206Z",
     "start_time": "2024-02-01T17:27:12.489392Z"
    }
   },
   "id": "5b89dbd42165ecf5"
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "outputs": [],
   "source": [
    "trainer = TransformerTrainer(train_args, model)\n",
    "# trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:27:16.248875300Z",
     "start_time": "2024-02-01T17:27:16.218206Z"
    }
   },
   "id": "8fa3b524155d0752"
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model with 600 epochs for comparison\n",
    "model600 = GPT(train_model_cfg).to(device)\n",
    "weights = t.load(\"gpt_model_weights600.pth\", map_location=t.device(\"cpu\"))\n",
    "model600.load_state_dict(weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:04.408477100Z",
     "start_time": "2024-02-01T17:37:03.905848600Z"
    }
   },
   "id": "97959f52e3a7ac9c"
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model with 800 epochs for comparison\n",
    "model800 = GPT(train_model_cfg).to(device)\n",
    "weights = t.load(\"gpt_model_weights800.pth\", map_location=t.device(\"cpu\"))\n",
    "model800.load_state_dict(weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:07.696489700Z",
     "start_time": "2024-02-01T17:37:06.353531200Z"
    }
   },
   "id": "ccfecd8f1ffa8d63"
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You never before those other.\" \" No-11 or get well done with you.\" \"10amance-467.\" \"It was just it thinking he was funny anyway.\" \"I didn't get a quite agree as a sunrise himself with the Steve chandeliers.\" \"What are there best with me?\" \"?\" \"It was that frosting nervous.\" \"Dan's seems that it blinded when you?\" He said that he came in this liking and an old Thirteenpx in the way.\" \"\n"
     ]
    }
   ],
   "source": [
    "string800 = \"You\"\n",
    "string600 = string800\n",
    "for i in range(100):\n",
    "    tokens = tokeniser([string800])[\"input_ids\"]\n",
    "    tokens = t.tensor(tokens).to(device)\n",
    "    logits = model800(tokens)\n",
    "    string800 += logits_to_strings(logits)\n",
    "\n",
    "print(string800)    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:45:02.526858700Z",
     "start_time": "2024-02-01T17:44:54.618858600Z"
    }
   },
   "id": "d40355455d8232be"
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You deserve to meet you haven't all the money, then fled subject?\"\n",
      "\n",
      "<|endoftext|>The port is onboard right?Do you know you and to pay for their pace. Certainly comes in five hours before daylight. Again, but the on your senses all every day.\" \"and tints.\" \"The in the morning.\" \"It is in, then ask Assumed by official sometime.\" \"She connected with her.\" \"[ Mickey 7800\" \"Ditch towards me.\" \"It was so interesting,\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    tokens = tokeniser([string600])[\"input_ids\"]\n",
    "    tokens = t.tensor(tokens).to(device)\n",
    "    logits = model600(tokens)\n",
    "    string600 += logits_to_strings(logits)\n",
    "\n",
    "print(string600)   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:45:07.558422900Z",
     "start_time": "2024-02-01T17:45:02.526858700Z"
    }
   },
   "id": "713643c5292a413c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5f48aae7eba2dc71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
